{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1) Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ML project offers a comprehensive solution to classify news articles into distinct political orientations, aiding users in understanding the underlying biases and perspectives shaping public discourse.\n",
    "\n",
    "The project comprises a series of intricately connected components:\n",
    "\n",
    "\n",
    "- *Scraping Applications*: url_scraping.py: This application retrieves a list of URLs containing news articles from diverse sources. news_scraping.py: Upon obtaining URLs, this application extracts the text content of the articles in a raw format.\n",
    "\n",
    "\n",
    "- *Data Cleaning* (cleaner.py): Once the text data is collected, the cleaner.py script plays a pivotal role in preprocessing the raw text, eliminating redundant information, such as HTML tags, advertisements, and other noise, to ensure the integrity and quality of the dataset.\n",
    "\n",
    "\n",
    "- *Data Preprocessing*: The collected text data undergoes preprocessing steps such as tokenization, stop-word removal, and possibly stemming or lemmatization to prepare it for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A series of ML models shall be proposed, evaluated and then compared amongst each other. Finally, simple tf-idf, neural embedding and hybrid search engines will be implemented for the same dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing standard libriaries\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt # for visualisation purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________\n",
    "**2) Preprocessing Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *OneHotEcondoer*: preprocessing technique that converts categorical variables into a numerical format that machine learning algorithms can understand. It takes categorical data, then creates binary columns for each category, and, finally, for each sample, puts 1 in the column matching its category and 0 in others.\n",
    "\n",
    "- *ColumnTransformer*: applies different transformations to different columns of your dataset in a single step.It nsures all transformations are applied consistently during training and prediction. It is quite useful as real datasets often have mixed types (ex. numeric, categorical, text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprting required libraries from skleanr\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('datasets/news.json').drop('source',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>orientation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Health authorities in one state have issued an...</td>\n",
       "      <td>western_conservative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n'Kennedy Saves the World' podcast host Kenne...</td>\n",
       "      <td>western_conservative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nFormer counterterrorism analyst Jonathan Sch...</td>\n",
       "      <td>western_conservative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nFox News Flash top headlines are here. Check...</td>\n",
       "      <td>western_conservative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\nCrowe is charged with harassment and stalkin...</td>\n",
       "      <td>western_conservative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article           orientation\n",
       "0  Health authorities in one state have issued an...  western_conservative\n",
       "1  \\n'Kennedy Saves the World' podcast host Kenne...  western_conservative\n",
       "2  \\nFormer counterterrorism analyst Jonathan Sch...  western_conservative\n",
       "3  \\nFox News Flash top headlines are here. Check...  western_conservative\n",
       "4  \\nCrowe is charged with harassment and stalkin...  western_conservative"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1164 entries, 0 to 1163\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   article      1164 non-null   object\n",
      " 1   orientation  1164 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 18.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['western_conservative', 'non_western', 'western_progressive'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['orientation'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a total of three unique categories: western conservative, non-western and western progressive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0, 1.0, 0.0,\n",
       "        'Health authorities in one state have issued an urgent alert for residents who visited a Costco, DFO, businesses and caught trams after two measles cases were infectious while in public.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nVictorian residents have been put on alert after two holidaymakers returning from overseas were unknowingly infectious with measles while out in the community.\\nThe Department of Health revealed the new cases on Saturday afternoon, which brings the total measles cases to three after another traveller was identified this week.\\nAt least 10 exposure sites have been listed, with the days ranging between Wednesday January 17 and Wednesday January 24, on the department\\'s website.\\nWant more news? Stream Sky News Australia’s live channel here. \\nWednesday January 17 \\n6am to 3pm: Bay City Auto Group (and associated construction site) 14 Dandenong Road West, Frankston\\n7:30pm to 9pm: Box Hill Action Indoor Sports 9 Clarice Road, Box Hill\\nThursday January 18\\n6am to 3pm: Bay City Auto Group (and associated construction site) 14 Dandenong Road West, Frankston\\nFriday January 19\\n6am to 3pm: Bay City Auto Group (and associated construction site) 14 Dandenong Road West, Frankston\\nMonday January 22\\n5:30pm to 7:20pm: Costco Ringwood, 29 Bond St, Ringwood\\nYou can now subscribe to stream four news channels live, and watch full Sky News Australia shows on demand, with the Sky News Australia app on mobile and tablet devices.\\nTerms and conditions apply. Content accessible in Australia only.\\nTuesday January 23\\n12 to 12:45pm: DFO South Wharf - shopping centre and carpark, 20 Convention Centre Pl\\n12:15pm to 2:30pm: Docklands Park Playground, 1-47 Harbour Esplanade, Docklands\\n12:40pm to 1:40pm: Collins Square Food Court, 727 Collin St, Docklands\\n1:10pm to 1:40pm: Tram 48 to North Balwyn from Stop 1 Spencer St/Collins St to Stop 5 Elizabeth St/Collins St #5\\n1:40pm to 3:35pm: Sea Life Melbourne Aquarium King St\\n3:30pm to 5:30pm: Melbourne River Cruises 4pm departure, Berth 2, Federation Wharf, Princes Walk, Melbourne\\n5:30pm to 6pm: tram 70 to Waterfront City Docklands. From Stop 6 Russell Street/Flinders St to Stop D6-Victoria Police Centre/Flinders St\\n5:40pm to 6:20pm: DFO South Wharf - carpark only, 20 Convention Centre Place\\nPeople who visited any of the places mentioned have been asked to monitor symptoms up to February 4 right through to February 10 depending on the date.\\nSome of the symptoms associated with measles includes flu-like signs such as runny nose, red eyes and a cough, followed by a fever or rash.\\nA rash then appears three to four days later, which most often starts on the face and then spreads to the rest of the body.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nVictoria’s Acting Chief Health Officer Dr Christian McGrath said the MMR vaccine can be given within 72 hours of exposure to prevent infection.\\n\"Failing that, we recommend that people monitor for symptoms and seek medical attention if any symptoms develop,\" he said in a statement.\\nRecent cases of measles have been found in people who were not fully vaccinated and had been infected while travelling overseas.\\nThose planning on international travel anytime soon are encouraged to see their doctor to get the MRR vaccine.\\nMore information on measles can be found on the Better Health Channel website.\\n\\n\\nOur Apps'],\n",
       "       [0.0, 1.0, 0.0,\n",
       "        '\\n\\'Kennedy Saves the World\\' podcast host Kennedy and Fox News contributor Tyrus join ‘America’s Newsroom’ to discuss Swift’s alleged stalker David Crowe and Ring no longer providing videos to police.\\nTaylor Swift continued to be the most talked about woman on the Internet on Sunday, when the Kansas City Chiefs were declared Super Bowl bound.\\nDespite Travis Kelce and Swift being at the forefront of trends, X still had her name blocked from the search function on its platform.\\nThe decision to keep Taylor\\'s name unsearchable on X began earlier in the week after it was discovered that sexually explicit AI-generated images were circulating on the social media network. \\nTAYLOR SWIFT AI-GENERATED EXPLICIT PHOTOS OUTRAGE FANS: ‘PROTECT TAYLOR SWIFT’\\nTaylor Swift searches were blocked from X amid an overwhelming number of fake images. (Getty Images)\\nRepresentatives for X did not immediately respond to Fox News Digital\\'s request for comment.\\nAs of Monday morning, Swift\\'s name was still not searchable on the platform. Users could still search for \"Taylor\" and \"Swift\" individually.\\nThe fake pornographic images made the rounds on Thursday, with X suspending at least one account associated with the images.\\nTAYLOR SWIFT\\'S ALLEGED STALKER ACCUSED OF VISITING HER NYC HOME 30 TIMES\\nX seemed to respond to the backlash with a statement shared by their safety team notifying users that posting \"non-consensual nudity (NCN) images is strictly prohibited on X and we have a zero-tolerance policy towards such content.\" \\nTaylor Swift\\'s name was still unsearchable on X as of Monday morning. (X)\\nLIKE WHAT YOU’RE READING? CLICK HERE FOR MORE ENTERTAINMENT NEWS\\n\"Our teams are actively removing all identified images and taking appropriate actions against the accounts responsible for posting them,\" they said. \"We\\'re closely monitoring the situation to ensure that any further violations are immediately addressed, and the content is removed. We\\'re committed to maintaining a safe and respectful environment for all users.\"\\nThe White House reacted Friday to the explicit images that went viral of Swift and leaned on Congress for a legislative crackdown.\\nCLICK HERE TO SIGN UP FOR THE ENTERTAINMENT NEWSLETTER\\n\"We are alarmed by the reports of the circulation of images … of false images to be more exact. And it is alarming,\" White House press secretary Karine Jean-Pierre said during a press briefing. \\nTaylor Swift was backed by the White House and SAG-AFTRA. (Gareth Cattermole)\\n\"So while social media companies make their own independent decisions about content management, we believe they have an important role to play in enforcing their own rules to prevent the spread of misinformation and non-consensual, intimate imagery of real people.\" \\nThe SAG-AFTRA actors union denounced the false images of Swift as \"upsetting\" and \"deeply concerning.\"\\nCLICK HERE TO GET THE FOX NEWS APP\\n\"The development and dissemination of fake images — especially those of a lewd nature — without someone’s consent must be made illegal,\" the union said in a statement. \"As a society, we have it in our power to control these technologies, but we must act now before it is too late.\" \\nThey added, \"We support Taylor, and women everywhere who are the victims of this kind of theft of their privacy and right to autonomy.\"\\nTracy Wright is an entertainment reporter for Fox News Digital. Send story tips to Tracy.Wright@fox.com.\\nWho\\'s making headlines in television, music, movies and more from Hollywood to the Heartland.\\n\\n\\nThis material may not be published, broadcast, rewritten, or redistributed. ©2024 FOX News Network, LLC. All rights reserved. Quotes displayed in real-time or delayed by at least 15 minutes. Market data provided by Factset. Powered and implemented by FactSet Digital Solutions. Legal Statement. Mutual Fund and ETF data provided by Refinitiv Lipper.'],\n",
       "       [0.0, 1.0, 0.0,\n",
       "        '\\nFormer counterterrorism analyst Jonathan Schanzer joins ‘Fox & Friends Weekend’ to discuss the latest reports that the UNRWA allegedly was involved in Hamas’ October 7 attack. \\nIsrael has provided the Biden administration with a new dossier with information about how staffers for a United Nations agency assisted or supported the Hamas terror attacks on Oct. 7, Fox News has learned. \\nThe dossier specifically alleges that 12 employees who worked with the United Nations Relief and Works Agency for Palestine Refugees in the Near East (UNRWA) aided in different capacities. According to the dossier, seven UN staffers crossed into Israel on Oct. 7 while other were accused of \"participating in a terror activity\" or coordinating vehicle movements.\\nThe dossier alleged that some 190 employees of UNRWA in Gaza had ties to Islamist terror groups; however, intelligence estimates provided to The Wall Street Journal put the number at approximately 1,200, or 10% of UNRWA\\'s Gaza workforce.\\nThe document says two joined Hamas terrorists in raiding an Israeli kibbutz and directly participated in violence, another two staffers kidnapped an Israeli woman, holding her hostage in their personal home; and, another UN staffer doled out ammunition to the Hamas terrorists.\\nAUSTRIA SUSPENDS PAYMENTS TO UNRWA AMID ISRAELI ALLEGATIONS UN WORKERS HELPED, CELEBRATED HAMAS\\nAn Israeli soldier patrols near Kibbutz Beeri in southern Israel on October 12, 2023, close to the place where 270 Israelis were killed by militants during the Supernova music festival on October 7. Thousands of people, both Israeli and Palestinians have died since October 7, 2023, after Palestinian Hamas militants entered Israel in a surprise attack leading Israel to declare war on Hamas in the Gaza Strip enclave the following day.  (ARIS MESSINIS/AFP via Getty Images)\\nThe dossier specifies the UN workers involved, providing their photos and a description of their roles at the agency. \\nUN Secretary-General Antonio Guterres said the individuals who were alleged to have been involved in the attack are no longer employed. \\n\"Of the 12 people implicated, nine were immediately identified and terminated by the Commissioner-General of UNRWA, Philippe Lazzarini; one is confirmed dead, and the identity of the two others is being clarified,\" said Guterres.\\nFILE PHOTO: A truck, marked with United Nations Relief and Works Agency (UNRWA) logo, crosses into Egypt from Gaza, at the Rafah border crossing between Egypt and the Gaza Strip, during a temporary truce between Hamas and Israel, in Rafah, Egypt, November 27, 2023.  (REUTERS/Amr Abdallah Dalsh/File Photo)\\nHe also said any UN employee that is found to have been involved in terror acts \"will be held accountable, including through criminal prosecution.\"\\nTRUMP ADMIN CUT FUNDING TO UN AGENCY NOW ACCUSED OF PARTICIPATING IN HAMAS ATTACK ‘FOR REASON’: REP. MCCAUL\\nOf the 12 workers, nine were teachers and one a social worker. Ten were specifically listed as having ties to Hamas and one to the Islamic Jihad terrorist group.\\nPolice Officer walks near a police station that was destroyed after a battle between Israeli troops and Hamas militants that have take the station  on October 8, 2023 in Sderot, Israel.  (Amir Levy/Getty Images)\\nThe allegations against the UNRWA staffers ignited worldwide backlash and prompted a dozen Western countries to suspend payments to the agency.\\nThe countries that paused payments to the aid agency include: the United States, the United Kingdom, France, Germany, Italy, Australia, Finland, the Netherlands, Switzerland, Canada, Japan and Austria.\\nUN CALLS ON COUNTRIES TO RESUME UNRWA FUNDING DESPITE REPORT EMPLOYEES PARTICIPATED IN OCT 7 MASSACRE\\nAustria was the most recent country to join the list, announcing their decision on Monday.\\n\"We call on UNRWA and the United Nations to conduct a comprehensive, swift and complete investigation into the allegations,\" the Austrian ministry said in a statement.\\nA picture taken from the southern Israeli city of Sderot on October 25, 2023, shows smoke ascending over the northern Gaza Strip following an Israeli strike, amid the ongoing battles between Israel and the Palestinian group Hamas.  (RONALDO SCHEMIDT/AFP via Getty Images)\\nThe contribution from these countries made up more than 60% of UNRWA’s total budget in 2022.\\nUNRWA provides basic services for millions of Palestinians across the Middle East but, with the majority of their financial support now in doubt, the agency said it will be forced to halt operations within weeks.\\nVisitors look at photos of Israeli people who were killed during Hamas militants attack on Oct. 7 and those who died during the Israel-Hamas war in the Gaza Strip, displayed on a giant screen at the National Library in Jerusalem, Israel, Sunday, Jan. 28, 2024.  (AP Photo/Leo Correa)\\nCLICK HERE TO GET THE FOX NEWS APP\\nCommunications Director Juliette Touma said if the funding is not restored, then the agency could be forced to stop its support in Gaza by the end of February.\\nFox News\\' Yonat Friling, Gillian Turner and The Associated Press contributed to this report.\\nFox News\\' \"Antisemitism Exposed\" newsletter brings you stories on the rising anti-Jewish prejudice across the U.S. and the world.\\n\\n\\nThis material may not be published, broadcast, rewritten, or redistributed. ©2024 FOX News Network, LLC. All rights reserved. Quotes displayed in real-time or delayed by at least 15 minutes. Market data provided by Factset. Powered and implemented by FactSet Digital Solutions. Legal Statement. Mutual Fund and ETF data provided by Refinitiv Lipper.']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn categories into numbers\n",
    "\n",
    "# Define the categorical features\n",
    "categorical_features = ['orientation']\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "one_hot = OneHotEncoder()\n",
    "\n",
    "# Initialize the ColumnTransformer\n",
    "transformer = ColumnTransformer([('one_hot', one_hot, categorical_features)], remainder='passthrough')\n",
    "\n",
    "# Apply the transformation to your dataframe\n",
    "df_transformed = transformer.fit_transform(df)\n",
    "df_transformed[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>western_conservative</th>\n",
       "      <th>non_western</th>\n",
       "      <th>western_progressive</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>health authorities in one state have issued an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>kennedy saves the world podcast host kennedy a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>former counterterrorism analyst jonathan schan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>fox news flash top headlines are here check ou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>crowe is charged with harassment and stalking ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  western_conservative non_western western_progressive  \\\n",
       "0                  0.0         1.0                 0.0   \n",
       "1                  0.0         1.0                 0.0   \n",
       "2                  0.0         1.0                 0.0   \n",
       "3                  0.0         1.0                 0.0   \n",
       "4                  0.0         1.0                 0.0   \n",
       "\n",
       "                                             article  \n",
       "0  health authorities in one state have issued an...  \n",
       "1  kennedy saves the world podcast host kennedy a...  \n",
       "2  former counterterrorism analyst jonathan schan...  \n",
       "3  fox news flash top headlines are here check ou...  \n",
       "4  crowe is charged with harassment and stalking ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turning it into a dataset format\n",
    "data = pd.DataFrame(df_transformed)\n",
    "data.columns = ['western_conservative','non_western','western_progressive','article'] # renaming the columns\n",
    "\n",
    "# Removing \\n and pre-word-embedding cleaning\n",
    "char = '\\\\'\n",
    "data['article'] = data['article'].str.replace('\"','')\n",
    "data['article'] = data['article'].str.replace(\"'\",\"\")\n",
    "data['article'] = data['article'].str.replace(',','')\n",
    "data['article'] = data['article'].str.replace('.','')\n",
    "data['article'] = data['article'].str.lower()\n",
    "data['article'] = data['article'].str.replace('\\n','')\n",
    "data['article'] = data['article'].str.replace(char,'')\n",
    "data['article'] = data['article'].str.replace('/','')\n",
    "data['article'] = data['article'].str.replace('—','')\n",
    "data['article'] = data['article'].str.replace('_','')\n",
    "data['article'] = data['article'].str.replace('’','')\n",
    "data['article'] = data['article'].str.replace('-','')\n",
    "data['article'] = data['article'].str.replace('@','')\n",
    "data['article'] = data['article'].str.replace('–','')\n",
    "data['article'] = data['article'].str.replace('‘','')\n",
    "data['article'] = data['article'].str.replace('…','')\n",
    "data['article'] = data['article'].str.replace('”','')\n",
    "data['article'] = data['article'].str.replace('“','')\n",
    "data['article'] = data['article'].str.replace(':','')\n",
    "data['article'] = data['article'].str.replace('!','')\n",
    "data['article'] = data['article'].str.replace('?','')\n",
    "data['article'] = data['article'].str.replace('^','')\n",
    "data['article'] = data['article'].str.replace('<','')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stopwrods for word-embedding\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['article'] = data['article'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>western_conservative</th>\n",
       "      <th>non_western</th>\n",
       "      <th>western_progressive</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>health authorities one state issued urgent ale...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>kennedy saves world podcast host kennedy fox n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>former counterterrorism analyst jonathan schan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>fox news flash top headlines check whats click...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>crowe charged harassment stalking related acti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  western_conservative non_western western_progressive  \\\n",
       "0                  0.0         1.0                 0.0   \n",
       "1                  0.0         1.0                 0.0   \n",
       "2                  0.0         1.0                 0.0   \n",
       "3                  0.0         1.0                 0.0   \n",
       "4                  0.0         1.0                 0.0   \n",
       "\n",
       "                                             article  \n",
       "0  health authorities one state issued urgent ale...  \n",
       "1  kennedy saves world podcast host kennedy fox n...  \n",
       "2  former counterterrorism analyst jonathan schan...  \n",
       "3  fox news flash top headlines check whats click...  \n",
       "4  crowe charged harassment stalking related acti...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving cleaned data\n",
    "data.to_csv('news_clean.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________\n",
    "\n",
    "***3) Word-Embedding***\n",
    "\n",
    "\n",
    "Word embedding is a technique that represents words as dense numerical vectors in a continuous vector space, capturing semantic and syntactic relationships between words based on their context in large datasets. Unlike traditional methods like one-hot encoding, word embeddings place similar words closer together in the vector space, allowing models to generalize better by understanding analogies, synonyms, and other linguistic patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Word2Vec*: a word embedding technique that uses a shallow neural network to learn vector representations of words based on their contextual usage in large text datasets. It captures semantic and syntactic relationships by predicting either a target word from its neighbors (Continuous Bag of Words, CBOW) or predicting surrounding words from a target word (Skip-gram). The resulting dense vectors place similar words (e.g., \"king\" and \"queen\") close together in the vector space, enabling tasks like analogy solving (e.g., \"king - man + woman ≈ queen\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1162, 4)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('datasets/news_clean.csv').dropna()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# tokenizing text data\n",
    "tokenized_data = [simple_preprocess(article) for article in data['article']]\n",
    "\n",
    "# training Word2Vec model\n",
    "# using recommended parameters\n",
    "word2vec_model = Word2Vec(sentences=tokenized_data, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "\n",
    "# retrieving word vectors for each token in the article\n",
    "word_vectors = []\n",
    "for tokens in tokenized_data:\n",
    "    vectors = [word2vec_model.wv[token] for token in tokens if token in word2vec_model.wv]\n",
    "    if vectors:\n",
    "        article_vector = sum(vectors) / len(vectors)  # average the word vectors to get one vector per article\n",
    "        word_vectors.append(article_vector)\n",
    "    else:\n",
    "        word_vectors.append(None)  # handle case where all tokens are out-of-vocabulary\n",
    "\n",
    "# converting word_vectors to pandas series\n",
    "word_vectors_series = pd.Series(word_vectors, name='word_embeddings')\n",
    "\n",
    "# adding  the word vectors as a new column in your DataFrame\n",
    "data['word_embeddings'] = word_vectors_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>western_conservative</th>\n",
       "      <th>non_western</th>\n",
       "      <th>western_progressive</th>\n",
       "      <th>article</th>\n",
       "      <th>word_embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>health authorities one state issued urgent ale...</td>\n",
       "      <td>[-0.59863424, 0.13775885, 0.012549366, -0.4808...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>kennedy saves world podcast host kennedy fox n...</td>\n",
       "      <td>[-0.85196894, 0.1582429, 0.08021765, -0.659584...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>former counterterrorism analyst jonathan schan...</td>\n",
       "      <td>[-0.6727169, 0.16773795, 0.027970431, -0.64749...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>fox news flash top headlines check whats click...</td>\n",
       "      <td>[-0.9936382, 0.21699953, -0.15789154, -0.89784...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>crowe charged harassment stalking related acti...</td>\n",
       "      <td>[-0.69958556, 0.20131096, 0.039057776, -0.5881...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   western_conservative  non_western  western_progressive  \\\n",
       "0                   0.0          1.0                  0.0   \n",
       "1                   0.0          1.0                  0.0   \n",
       "2                   0.0          1.0                  0.0   \n",
       "3                   0.0          1.0                  0.0   \n",
       "4                   0.0          1.0                  0.0   \n",
       "\n",
       "                                             article  \\\n",
       "0  health authorities one state issued urgent ale...   \n",
       "1  kennedy saves world podcast host kennedy fox n...   \n",
       "2  former counterterrorism analyst jonathan schan...   \n",
       "3  fox news flash top headlines check whats click...   \n",
       "4  crowe charged harassment stalking related acti...   \n",
       "\n",
       "                                     word_embeddings  \n",
       "0  [-0.59863424, 0.13775885, 0.012549366, -0.4808...  \n",
       "1  [-0.85196894, 0.1582429, 0.08021765, -0.659584...  \n",
       "2  [-0.6727169, 0.16773795, 0.027970431, -0.64749...  \n",
       "3  [-0.9936382, 0.21699953, -0.15789154, -0.89784...  \n",
       "4  [-0.69958556, 0.20131096, 0.039057776, -0.5881...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Right now the arrays are Series containing whole strings\n",
    "# Converting to lists with floats:\n",
    "\n",
    "# Check the type of the first element\n",
    "print(type(data['word_embeddings'].iloc[0]))\n",
    "\n",
    "# If it's already a NumPy array, convert it to a list directly\n",
    "if isinstance(data['word_embeddings'].iloc[0], np.ndarray):\n",
    "    data['word_embeddings'] = data['word_embeddings'].apply(lambda x: x.tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data['word_embeddings'][0])\n",
    "# List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data['word_embeddings'][0][0])\n",
    "# Float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the word-embedded dataframe\n",
    "data.to_csv('news_embedded.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________________\n",
    "**4) Model Fitting and Evaluation**\n",
    "\n",
    "The problem is a Text Classification Problem. \n",
    "\n",
    "For this kind of problem, the following suitable models will be applied and evaluated:\n",
    "\n",
    "- Naive Bayes\n",
    "\n",
    "- Support Vector Machines (SVM)\n",
    "\n",
    "- Random Forest or Gradient Boosting Machines\n",
    "\n",
    "- Neural Networks: deep learning models like Convolutional Neural Networks (CNNs) or Recurrent Neural Networks (RNNs).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I) ***Naive Bayes***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('datasets/news_embedded.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Concatenating the label columns into a single label column\n",
    "data['label'] = data[['western_conservative', 'non_western', 'western_progressive']].idxmax(axis=1)\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['article'], data['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorizing the text data\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "# Create the model and fitting it to the data\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = nb_classifier.predict(X_test_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9267241379310345\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article                             former president donald trump seeking sweeping...\n",
      "Predicted Label                                                   western_progressive\n",
      "non_western Probability                                                           0.0\n",
      "western_conservative Probability                                                  0.0\n",
      "western_progressive Probability                                                   1.0\n",
      "Name: 865, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Getting predicted probabilities\n",
    "proba = nb_classifier.predict_proba(X_test_vectorized)\n",
    "\n",
    "# Creating a DataFrame to display results\n",
    "results_data = pd.DataFrame({'Article': X_test, 'Predicted Label': y_pred})\n",
    "for i, label in enumerate(nb_classifier.classes_):\n",
    "    results_data[label + ' Probability'] = proba[:, i]\n",
    "\n",
    "# Printing the results\n",
    "print(results_data.iloc[90])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "II) ***Support Vector Machine***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.9827586206896551\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Article                             former president donald trump seeking sweeping...\n",
       "Predicted Label                                                   western_progressive\n",
       "non_western Probability                                                      0.003567\n",
       "western_conservative Probability                                             0.001326\n",
       "western_progressive Probability                                              0.995107\n",
       "Name: 865, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC # Importing SVC \n",
    "\n",
    "# Create and train the SVM model\n",
    "svm_classifier = SVC(kernel='linear', probability=True) # You can experiment with different kernels\n",
    "svm_classifier.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_svm = svm_classifier.predict(X_test_vectorized)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "print(\"SVM Accuracy:\", accuracy_svm)\n",
    "\n",
    "# Getting predicted probabilities\n",
    "proba_svm = svm_classifier.predict_proba(X_test_vectorized)\n",
    "\n",
    "# Creating a DataFrame to display results\n",
    "results_svm = pd.DataFrame({'Article': X_test, 'Predicted Label': y_pred_svm})\n",
    "for i, label in enumerate(svm_classifier.classes_):\n",
    "    results_svm[label + ' Probability'] = proba_svm[:, i]\n",
    "\n",
    "# Printing the results\n",
    "results_svm.iloc[90]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "III) ***Random Forest***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.978448275862069\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Article                             former president donald trump seeking sweeping...\n",
       "Predicted Label                                                   western_progressive\n",
       "non_western Probability                                                          0.01\n",
       "western_conservative Probability                                                  0.0\n",
       "western_progressive Probability                                                  0.99\n",
       "Name: 865, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "\n",
    "\n",
    "# Create and train the Random Forest model\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42) # You can adjust n_estimators\n",
    "rf_classifier.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = rf_classifier.predict(X_test_vectorized)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(\"Random Forest Accuracy:\", accuracy_rf)\n",
    "\n",
    "# Getting predicted probabilities\n",
    "proba_rf = rf_classifier.predict_proba(X_test_vectorized)\n",
    "\n",
    "# Creating a DataFrame to display results\n",
    "results_rf = pd.DataFrame({'Article': X_test, 'Predicted Label': y_pred_rf})\n",
    "for i, label in enumerate(rf_classifier.classes_):\n",
    "    results_rf[label + ' Probability'] = proba_rf[:, i]\n",
    "\n",
    "# Printing the results\n",
    "results_rf.iloc[90]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IV) ***Recurrent Neural Networks***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Marco\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Marco\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Marco\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:From C:\\Users\\Marco\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Marco\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "29/29 [==============================] - 7s 153ms/step - loss: 1.0279 - accuracy: 0.4817 - val_loss: 0.9698 - val_accuracy: 0.4224\n",
      "Epoch 2/5\n",
      "29/29 [==============================] - 4s 127ms/step - loss: 0.7949 - accuracy: 0.6304 - val_loss: 0.7056 - val_accuracy: 0.6207\n",
      "Epoch 3/5\n",
      "29/29 [==============================] - 4s 135ms/step - loss: 0.4218 - accuracy: 0.8631 - val_loss: 0.6127 - val_accuracy: 0.7328\n",
      "Epoch 4/5\n",
      "29/29 [==============================] - 4s 133ms/step - loss: 0.2258 - accuracy: 0.9289 - val_loss: 0.6301 - val_accuracy: 0.7543\n",
      "Epoch 5/5\n",
      "29/29 [==============================] - 4s 125ms/step - loss: 0.1537 - accuracy: 0.9591 - val_loss: 0.6051 - val_accuracy: 0.7802\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 0.6051 - accuracy: 0.7802\n",
      "RNN Accuracy: 0.7801724076271057\n",
      "8/8 [==============================] - 1s 38ms/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Assuming you have a vocabulary size and maximum sequence length\n",
    "vocab_size = 10000  \n",
    "max_len = 200      \n",
    "\n",
    "# Convert text to sequences of integers (tokenization)\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(data['article'])\n",
    "sequences = tokenizer.texts_to_sequences(data['article'])\n",
    "padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_len, truncating='post', padding='post')\n",
    "\n",
    "# One-hot encode the labels\n",
    "labels = pd.get_dummies(data['label']).values\n",
    "\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 128, input_length=max_len)) # Embedding layer\n",
    "model.add(LSTM(128)) # LSTM layer\n",
    "model.add(Dense(3, activation='softmax')) # Output layer (3 classes)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"RNN Accuracy:\", accuracy)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 35ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Article                             36.000000\n",
       "Predicted Label                      2.000000\n",
       "non_western Probability              0.002677\n",
       "western_conservative Probability     0.001412\n",
       "western_progressive Probability      0.995911\n",
       "Name: 90, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get predicted probabilities\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Create a datset to display results\n",
    "results_rnn = pd.DataFrame({'Article': X_test[:, 0], 'Predicted Label': np.argmax(predictions, axis=1)}) # Assuming X_test contains tokenized sequences\n",
    "\n",
    "# Get class labels from the one-hot encoded training labels\n",
    "class_labels = list(pd.get_dummies(data['label']).columns)\n",
    "\n",
    "for i, label in enumerate(class_labels):\n",
    "    results_rnn[label + ' Probability'] = predictions[:, i]\n",
    "\n",
    "# Printing the results for the 90th sample\n",
    "results_rnn.iloc[90]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________________________\n",
    "**5) Search Engine Implementation** \n",
    "\n",
    "\n",
    "This code implements and evaluates three different search engines for political news articles, along with an evaluation system using LLM (Large Language Model) judgments.\n",
    "\n",
    "The following section has been built with the help of Stack Overflow articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import openai  # For LLM judge evaluation\n",
    "\n",
    "# Load the cleaned data\n",
    "data = pd.read_csv('datasets/news_embedded.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Threads used as reference:\n",
    "\n",
    "\n",
    "- https://stackoverflow.com/questions/23838056/what-is-the-difference-between-transform-and-fit-transform-in-sklearn\n",
    "\n",
    "\n",
    "\n",
    "- https://stackoverflow.com/questions/27697766/understanding-min-df-and-max-df-in-scikit-countvectorizer\n",
    "\n",
    "\n",
    "- https://stackoverflow.com/questions/18424228/cosine-similarity-between-2-number-lists\n",
    "\n",
    "\n",
    "- https://stackoverflow.com/questions/51485813/an-illegal-reflective-access-operation-has-occurred-while-setting-up-spring-xd/51486492#51486492\n",
    "\n",
    "\n",
    "- https://stackoverflow.com/questions/17901218/numpy-argsort-what-is-it-doing\n",
    "\n",
    "\n",
    "- https://stackoverflow.com/questions/26984414/efficiently-sorting-a-numpy-array-in-descending-order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDEngine:\n",
    "    def __init__(self, documents):\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.tfidf_matrix = self.vectorizer.fit_transform(documents) # Create TF-IDF matrix from documents\n",
    "        self.documents = documents # Store original documents\n",
    "        \n",
    "    def search(self, query, top_k=5):\n",
    "        query_vec = self.vectorizer.transform([query]) # # Convert query to TF-IDF vector\n",
    "        # Calculate cosine similarity between query and all documents\n",
    "        similarities = cosine_similarity(query_vec, self.tfidf_matrix).flatten()\n",
    "        # Get indices of top k most similar documents\n",
    "        top_indices = similarities.argsort()[-top_k:][::-1]\n",
    "        # Return top documents with their similarity scores\n",
    "        return [(self.documents.iloc[i], similarities[i]) for i in top_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Threads used as reference:\n",
    "\n",
    "\n",
    "- https://stackoverflow.com/questions/6910641/how-do-i-get-indices-of-n-maximum-values-in-a-numpy-array\n",
    "\n",
    "- https://stackoverflow.com/questions/51485813/an-illegal-reflective-access-operation-has-occurred-while-setting-up-spring-xd/51486492#51486492\n",
    "\n",
    "- https://stackoverflow.com/questions/55258608/get-values-from-list-of-tuples-according-to-first-value/55258680#55258680\n",
    "\n",
    "- https://stackoverflow.com/questions/70411258/image-tag-how-to-put-on-an-icon-not-string-not-url-in-case-of-url-failure\n",
    "\n",
    "- https://stackoverflow.com/questions/65419499/download-pre-trained-sentence-transformers-model-locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralEmbeddingSE:\n",
    "    def __init__(self, documents):\n",
    "        # Load pre-trained sentence transformer model\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        # Create embeddings for all documents\n",
    "        self.doc_embeddings = self.model.encode(documents.tolist())\n",
    "        self.documents = documents\n",
    "        \n",
    "    def search(self, query, top_k=5):\n",
    "        query_embedding = self.model.encode([query])  # Create embedding for query\n",
    "        # Calculate cosine similarity between query and document embeddings\n",
    "        similarities = cosine_similarity(query_embedding, self.doc_embeddings).flatten()\n",
    "        top_indices = similarities.argsort()[-top_k:][::-1]  # Get top indices\n",
    "        return [(self.documents.iloc[i], similarities[i]) for i in top_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Threads used as reference: \n",
    "\n",
    "- https://stackoverflow.com/questions/58662904/how-to-access-stdshared-ptr-methods/58663062#58663062\n",
    "\n",
    "- https://stackoverflow.com/questions/38640109/logistic-regression-python-solvers-definitions\n",
    "\n",
    "- https://stackoverflow.com/questions/613183/how-do-i-sort-a-dictionary-by-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "class HybridSE:\n",
    "    def __init__(self, documents):\n",
    "        # Initialize both engines\n",
    "        self.tfidf_engine = TFIDEngine(documents)\n",
    "        self.neural_engine = NeuralEmbeddingSE(documents)\n",
    "        self.documents = documents\n",
    "        \n",
    "    def search(self, query, top_k=5, alpha=0.5):\n",
    "        # Get results from both engines (twice as many as needed)\n",
    "        tfidf_results = self.tfidf_engine.search(query, top_k*2)\n",
    "        neural_results = self.neural_engine.search(query, top_k*2)\n",
    "        \n",
    "        # Combine scores with weighted average\n",
    "        combined_scores = {}\n",
    "        for doc, score in tfidf_results:\n",
    "            combined_scores[doc] = combined_scores.get(doc, 0) + alpha * score\n",
    "            \n",
    "        for doc, score in neural_results:\n",
    "            combined_scores[doc] = combined_scores.get(doc, 0) + (1 - alpha) * score\n",
    "            \n",
    "        # Sort by combined score and return top k\n",
    "        sorted_results = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        return sorted_results[:top_k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Threads: used as reference:\n",
    "\n",
    "- https://stackoverflow.com/questions/76256897/how-to-make-a-conditional-statement-using-two-different-dataframes-in-pandas/76257033#76257033\n",
    "\n",
    "- https://stackoverflow.com/questions/75203085/sanitys-urlfor-not-working-with-my-react-app\n",
    "\n",
    "- https://stackoverflow.com/questions/75329835/kotlin-json-string-with-linebreaker-and-variable\n",
    "\n",
    "- https://stackoverflow.com/questions/71992082/how-to-install-and-run-ward-monitoring-tool-on-linux-ubuntu/71992083#71992083"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMJudge:\n",
    "    def __init__(self, api_key):\n",
    "        openai.api_key = api_key\n",
    "        self.prompt_template = \"Query: {query}, Retrieved Document: {document}\" # Template for LLM evaluation\n",
    "     \n",
    "    def evaluate(self, query, document):\n",
    "        prompt = self.prompt_template.format(query=query, document=document)\n",
    "        try:\n",
    "            # Call OpenAI API to get relevance rating\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[{\"role\": \"system\", \"content\": prompt}],\n",
    "                max_tokens=2\n",
    "            )\n",
    "            rating = int(response.choices[0].message.content.strip())\n",
    "            return rating\n",
    "        except:\n",
    "            return 3  # Default neutral rating if API fails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Threads used as reference:\n",
    "\n",
    "- https://stackoverflow.com/questions/522563/how-can-i-access-the-index-value-in-a-for-loop\n",
    "\n",
    "- https://stackoverflow.com/questions/9039961/finding-the-average-of-a-list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_search_engine(engine, queries, judge, top_k=3):\n",
    "    # Evaluates an engine by getting average rating across all queries\n",
    "    scores = []\n",
    "    for query in queries:\n",
    "        results = engine.search(query, top_k=top_k)\n",
    "        for doc, _ in results:\n",
    "            score = judge.evaluate(query, doc)\n",
    "            scores.append(score)\n",
    "    return np.mean(scores) if scores else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Threads used as reference:\n",
    "\n",
    "- https://stackoverflow.com/questions/1798465/remove-last-3-characters-of-a-string\n",
    "\n",
    "- https://stackoverflow.com/questions/45310254/fixed-digits-after-decimal-with-f-strings\n",
    "\n",
    "- https://stackoverflow.com/questions/23267409/how-to-implement-retry-mechanism-into-python-requests-library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Marco\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sample Search Results ===\n",
      "\n",
      "Query: 'conservative view on immigration'\n",
      "\n",
      "TF-IDF Results:\n",
      "Score: 0.129 | oklahoma gov kevin stitt (r) says invasion occurring southern border applauds texas gov greg abbotta...\n",
      "\n",
      "Neural Embedding Results:\n",
      "Score: 0.418 | liveisraelhamas warlivetech ceos testify senateby aditi sangal elise hammond maureen chowdhury tori ...\n",
      "\n",
      "Hybrid Results:\n",
      "Score: 0.418 | liveisraelhamas warlivetech ceos testify senateby aditi sangal elise hammond maureen chowdhury tori ...\n",
      "\n",
      "Query: 'progressive economic policies'\n",
      "\n",
      "TF-IDF Results:\n",
      "Score: 0.071 | donald trumps voters get attention joe bidens may decide general electiona historic rematch white ho...\n",
      "\n",
      "Neural Embedding Results:\n",
      "Score: 0.377 | marketsfear & greed indexlatest market newsit happened us economy defied yet another forecast big wa...\n",
      "\n",
      "Hybrid Results:\n",
      "Score: 0.188 | marketsfear & greed indexlatest market newsit happened us economy defied yet another forecast big wa...\n"
     ]
    }
   ],
   "source": [
    "# Initialize search engines\n",
    "tfidf_se = TFIDEngine(data['article'])\n",
    "neural_se = NeuralEmbeddingSE(data['article'])\n",
    "hybrid_se = HybridSE(data['article'])\n",
    "\n",
    "# Sample test queries\n",
    "test_queries = [\n",
    "    \"conservative view on immigration\",\n",
    "    \"progressive economic policies\",\n",
    "    \"international relations from non-western perspective\",\n",
    "    \"healthcare reform debate\",\n",
    "    \"climate change policies\"\n",
    "]\n",
    "\n",
    "# a manual comparison for demonstration without API key \n",
    "def manual_evaluation():\n",
    "    print(\"=== Sample Search Results ===\")\n",
    "    for query in test_queries[:2]:  # Just show first 2 for demo\n",
    "        print(f\"\\nQuery: '{query}'\")\n",
    "        \n",
    "        print(\"\\nTF-IDF Results:\")\n",
    "        for doc, score in tfidf_se.search(query, top_k=1):\n",
    "            print(f\"Score: {score:.3f} | {doc[:100]}...\")\n",
    "        \n",
    "        print(\"\\nNeural Embedding Results:\")\n",
    "        for doc, score in neural_se.search(query, top_k=1):\n",
    "            print(f\"Score: {score:.3f} | {doc[:100]}...\")\n",
    "        \n",
    "        print(\"\\nHybrid Results:\")\n",
    "        for doc, score in hybrid_se.search(query, top_k=1):\n",
    "            print(f\"Score: {score:.3f} | {doc[:100]}...\")\n",
    "\n",
    "manual_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________________________\n",
    "**6) Conclusion** \n",
    "\n",
    "\n",
    "\n",
    "The implementation of the ML models in the third section yielded considerably positive results, with accuracy scores rarely falling below the 0.85 threshold. The model with the highest performance was SVM with an average score of 0.98, whereas the worst performance was obtained with the application of an RNN model with an average score of 0.78. Ultimately, the dataset contained news sources which may fall under the category of “mainstream media”, therefore, in order to further test the models’ predictive performance, it would be advisable to build new datasets containing a wider variety of news sources. Finally, less satisfactory were the results obtained by the search engine implementations. Even the Neural Embedding Engine, which yielded the best scores out the three engines proposed, failed nonetheless to surpass the 0.5 score threshold, thus showcasing a poor affinity between the data collected and the parameters adopted in the final evaluation.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
